{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b144d006-e638-4078-a1ef-45869b187da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Political candidate wordcloud - batch version\n",
    "\n",
    "This notebook takes a csv file with a list of links to candidate websites, scrapes the text, then generates a wordcloud from the text.\n",
    "\n",
    "The file \"url_list.csv\" should have urls in the first column, a language code in the second column (either 'en', 'fr', or 'all'), and a filename for saving the image in the third column.\n",
    "\n",
    "You can upload the file using the file browser in the left panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5194e2-c6ee-42c4-a838-d26dcae1756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER THE FILENAME WITH A LIST OF URLS\n",
    "# this expects the url in the 1st column, language in 2nd, and save name in 3rd\n",
    "filename = \"url_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d404828-5347-46af-822f-942da42e9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import requests\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe333ef-bf73-4aba-a863-dad2355eb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(url, min_len=3, max_len=30):\n",
    "    \"\"\"\n",
    "    This function scrapes the text from a url and returns a list of words without punctuation.\n",
    "    Words that don't contain any ascii letters are also removed (for example, this removes \n",
    "    words written in Inuktitut that are not rendered properly by the WordCloud package).\n",
    "    \n",
    "    Inputs:\n",
    "    url : the url for a website to be scraped\n",
    "    min_len : the minimum length for words to be included (default 3)\n",
    "    max_len : the maximum length for words to be included (default 30)\n",
    "    \n",
    "    Returns:\n",
    "    words : a single string with each extracted word separated by a space. This is the format  \n",
    "        used by the WordCloud package.\n",
    "    wordlist : a list of extracted words (same as `words`, just different format)\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    # parse \n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    words = \"\"\n",
    "    wordlist = []\n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation.append('’') # special apostrophe from websites\n",
    "    punctuation.append('…') # special ellipses from websites\n",
    "    \n",
    "    # iterate through the text and extract words\n",
    "    for line in text.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        for word in line.split(' '):\n",
    "            #print(word)\n",
    "            #word = word.strip()\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            #print(word)\n",
    "            if len(word) > max_len:\n",
    "                continue\n",
    "            if len(word) < min_len:\n",
    "                continue\n",
    "            # get words with an apostrophe\n",
    "            if \"'\" in word:\n",
    "                word_apostrophe = word.split(\"'\")\n",
    "                word = words_apostrophe[0]\n",
    "            elif \"’\" in word:\n",
    "                word_apostrophe = word.split(\"’\") # fun apostrophe\n",
    "                word = word_apostrophe[0]\n",
    "            # skip words with punctuation\n",
    "            check = False\n",
    "            for p in punctuation:\n",
    "                if p in word:\n",
    "                    check = True\n",
    "            if check == True:\n",
    "                continue\n",
    "            # skip words that don't have an ascii letter in them\n",
    "            letter_check = False\n",
    "            for l in string.ascii_letters:\n",
    "                if l in word:\n",
    "                    letter_check = True\n",
    "                    break\n",
    "            if letter_check == False:\n",
    "                continue\n",
    "\n",
    "            words += \" \"\n",
    "            words += word\n",
    "            wordlist.append(word)\n",
    "    \n",
    "    return words, wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5932d0c6-75c6-42d9-ad00-2caf8c3b8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e65b5b-f03b-44ee-93e7-9bca5abf2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set columns to expected names\n",
    "data.columns = ['url', 'language', 'save_name']\n",
    "data['language'] = data['language'].str.lower() # set all language fields to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e80d6fe-3090-45f5-8be3-f34fd0a1e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopwords\n",
    "# a list of french stopwords\n",
    "url = \"https://github.com/stopwords-iso/stopwords-fr/raw/master/stopwords-fr.txt\"\n",
    "stopwords_fr = set(requests.get(url).content.decode('utf-8').split('\\n'))\n",
    "stopwords_en = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73a03c-d9b6-4cf0-a1f5-30b36b02aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in data.groupby(['save_name', 'language']):\n",
    "    save_name = group[0][0]\n",
    "    language = group[0][1]\n",
    "    candidate = group[1]\n",
    "    print(save_name)\n",
    "\n",
    "    # iterate through multiple urls for the same candidate (if applicable)\n",
    "    all_words = \"\"\n",
    "    all_wordlist = []\n",
    "    for i, row in candidate.iterrows():\n",
    "        words, wordlist = get_words(row['url'])\n",
    "        all_words += words\n",
    "        all_words += \" \"\n",
    "        all_wordlist += wordlist\n",
    "\n",
    "    # set stopwords by language\n",
    "    if language == 'fr':\n",
    "        stopwords = stopwords_fr\n",
    "    elif language == 'en':\n",
    "        stopwords = stopwords_en\n",
    "    elif language == 'all':\n",
    "        stopwords = stopwords_fr.union(stopwords_en)\n",
    "        \n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(words)\n",
    " \n",
    "    # plot the WordCloud image                      \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.savefig(\"%s.png\" %save_name, dpi = 300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
